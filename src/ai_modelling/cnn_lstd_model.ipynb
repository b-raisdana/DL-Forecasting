{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:31:47.611701Z",
     "start_time": "2024-10-12T15:31:42.824119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from FigurePlotter.plotter import show_and_save_plot\n",
    "from PanderaDFM.OHLCV import MultiTimeframeOHLCV\n",
    "from helper.data_preparation import single_timeframe\n",
    "from helper.helper import date_range_to_string\n",
    "from Config import config\n",
    "\n",
    "config.processing_date_range = date_range_to_string(start=pd.to_datetime('03-01-24'),\n",
    "                                                    end=pd.to_datetime('09-01-24'))\n",
    "# devided by rolling mean, std\n",
    "n_mt_ohlcv = pd.read_csv(\n",
    "    os.path.join(r\"C:\\Code\\dl-forcasting\\data\\Kucoin\\Spot\\BTCUSDT\",\n",
    "                 f\"n_mt_ohlcv.{config.processing_date_range}.csv.zip\"), parse_dates=['date'], compression='zip')\n",
    "n_mt_ohlcv.set_index(['timeframe', 'date'], inplace=True, drop=True)\n",
    "n_mt_ohlcv.dtypes, n_mt_ohlcv.index.dtypes"
   ],
   "id": "9caca9023e737e7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(open      float64\n",
       " close     float64\n",
       " high      float64\n",
       " low       float64\n",
       " volume    float64\n",
       " dtype: object,\n",
       " timeframe                 object\n",
       " date         datetime64[ns, UTC]\n",
       " dtype: object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:05.255218Z",
     "start_time": "2024-10-12T15:32:05.070246Z"
    }
   },
   "cell_type": "code",
   "source": "n_mt_ohlcv.describe()",
   "id": "db6e26c4e54b9820",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                open          close           high            low  \\\n",
       "count  340668.000000  340668.000000  340668.000000  340668.000000   \n",
       "mean        0.002571       0.002537       0.000586       0.004814   \n",
       "std         1.080184       1.080047       1.081013       1.083676   \n",
       "min       -11.829250     -11.796538     -10.789105     -12.271370   \n",
       "25%        -0.556785      -0.557024      -0.575532      -0.531490   \n",
       "50%         0.010991       0.010835      -0.014932       0.036296   \n",
       "75%         0.575594       0.574331       0.553501       0.591531   \n",
       "max        13.730087      13.773854      13.686195      12.303279   \n",
       "\n",
       "              volume  \n",
       "count  340668.000000  \n",
       "mean        0.011528  \n",
       "std         1.070377  \n",
       "min        -4.911268  \n",
       "25%        -0.405402  \n",
       "50%        -0.136793  \n",
       "75%         0.143095  \n",
       "max        15.969589  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340668.000000</td>\n",
       "      <td>340668.000000</td>\n",
       "      <td>340668.000000</td>\n",
       "      <td>340668.000000</td>\n",
       "      <td>340668.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.011528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.080184</td>\n",
       "      <td>1.080047</td>\n",
       "      <td>1.081013</td>\n",
       "      <td>1.083676</td>\n",
       "      <td>1.070377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-11.829250</td>\n",
       "      <td>-11.796538</td>\n",
       "      <td>-10.789105</td>\n",
       "      <td>-12.271370</td>\n",
       "      <td>-4.911268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.556785</td>\n",
       "      <td>-0.557024</td>\n",
       "      <td>-0.575532</td>\n",
       "      <td>-0.531490</td>\n",
       "      <td>-0.405402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.010991</td>\n",
       "      <td>0.010835</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>-0.136793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.575594</td>\n",
       "      <td>0.574331</td>\n",
       "      <td>0.553501</td>\n",
       "      <td>0.591531</td>\n",
       "      <td>0.143095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.730087</td>\n",
       "      <td>13.773854</td>\n",
       "      <td>13.686195</td>\n",
       "      <td>12.303279</td>\n",
       "      <td>15.969589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multi timeframe modelling\n",
    "\n",
    "\n",
    "structure_timeframes = {\n",
    "    '1W':{        pattern: '1D',        trigger: '4h',        double: '15min',    }, \n",
    "    '1D':{        pattern: '4h',        trigger: '1h',        double: '5min',    }, \n",
    "    '4h':{        pattern: '1h',        trigger: '15min',        double: '1min',    }, \n",
    "}\n",
    "n_mt_ohlcv include open, high, low, close, and volume of all timeframes.\n",
    "single_timeframe(n_mt_ohlcv, timeframe) will return data of specified timeframe.\n",
    "using tensorflow create 4 parallel CNN-LSTM models each fed with structure, pattern, trigger, and double timeframe data.\n",
    "join these parallel models together.\n"
   ],
   "id": "6427f865497c1ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:10.169024Z",
     "start_time": "2024-10-12T15:32:10.152511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Conv1D, LeakyReLU, Flatten, Dense, Concatenate\n",
    "from tensorflow.python.keras import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "model_input_lengths = {\n",
    "    'structure': 128,\n",
    "    'pattern': 256,\n",
    "    'trigger': 256,\n",
    "    'double': 256,\n",
    "}\n",
    "\n",
    "\n",
    "def create_cnn_lstm(input_shape, name_prefix):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # CNN Layer with ReLU activation\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same')(input_layer)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same')(conv)\n",
    "    conv = LeakyReLU()(conv)\n",
    "\n",
    "    # Flatten the CNN output\n",
    "    flatten = Flatten()(conv)\n",
    "\n",
    "    # LSTM Layer (LSTM has built-in activations)\n",
    "    lstm = LSTM(64, return_sequences=False)(tf.expand_dims(flatten, axis=1))\n",
    "\n",
    "    # Fully connected layer with ReLU activation\n",
    "    dense = Dense(64)(lstm)\n",
    "    dense = LeakyReLU()(dense)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=dense)\n",
    "\n",
    "\n",
    "def build_model(input_shapes):\n",
    "    structure_model = create_cnn_lstm((model_input_lengths['structure'], 5), 'structure_model')\n",
    "    pattern_model = create_cnn_lstm((model_input_lengths['pattern'], 5), 'pattern_model')\n",
    "    trigger_model = create_cnn_lstm((model_input_lengths['trigger'], 5), 'trigger_model')\n",
    "    double_model = create_cnn_lstm((model_input_lengths['double'], 5), 'double_model')\n",
    "\n",
    "    combined_output = Concatenate()(\n",
    "        [structure_model.output, pattern_model.output, trigger_model.output, double_model.output])\n",
    "\n",
    "    # Add an additional Dense layer with ReLU activation\n",
    "    combined_dense = Dense(128)(combined_output)\n",
    "    combined_dense = LeakyReLU()(combined_dense)\n",
    "\n",
    "    # Final output layer (for regression tasks, use linear activation; for classification, consider sigmoid/softmax)\n",
    "    final_output = Dense(1, activation='linear')(combined_dense)\n",
    "\n",
    "    # Define the final model\n",
    "    model = Model(inputs=[structure_model.input, pattern_model.input, trigger_model.input, double_model.input],\n",
    "                  outputs=final_output)\n",
    "\n",
    "    # Compile the model with mean squared error loss for regression tasks\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n"
   ],
   "id": "a1f1715472a3fc2b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Check if model is not trained yet, try loading it from 'cnn_lstm_model.h5'. \n",
    "If it is already partially trained, or loaded from disk, continue training.\n",
    "after completing training on each set of data save model into 'cnn_lstm_model.h5' to prevent loosing data in case of computer restart.\n"
   ],
   "id": "8ce4bfcdb05bde50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:10.194325Z",
     "start_time": "2024-10-12T15:32:10.183021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "\n",
    "def train_model(structure_data, pattern_data, trigger_data, double_data, target_data, input_shapes, model=None):\n",
    "    '''\n",
    "    Check if the model is already trained or partially trained. If not, build a new model. \n",
    "    Continue training the model and save the trained model to 'cnn_lstm_model.h5' after each session.\n",
    "\n",
    "    Args:\n",
    "        structure_data: Data for the structure timeframe.\n",
    "        pattern_data: Data for the pattern timeframe.\n",
    "        trigger_data: Data for the trigger timeframe.\n",
    "        double_data: Data for the double timeframe.\n",
    "        target_data: The labels or target values for training.\n",
    "        input_shapes: A dictionary containing the input shapes for structure, pattern, trigger, and double timeframe data.\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    '''\n",
    "    # Check if the model already exists, load if it does\n",
    "    model_path = 'cnn_lstm_model.h5'\n",
    "\n",
    "    if model is None:\n",
    "        if os.path.exists(model_path):\n",
    "            print(\"Loading existing model from disk...\")\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print(\"Building new model...\")\n",
    "            model = build_model(input_shapes)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([structure_data, pattern_data, trigger_data, double_data],\n",
    "                        target_data,\n",
    "                        epochs=10,\n",
    "                        batch_size=32)\n",
    "    print(history)\n",
    "    # Save the model after each training session to avoid losing progress\n",
    "    model.save(model_path)\n",
    "    print(\"Model saved to disk.\")\n",
    "\n",
    "    return model"
   ],
   "id": "f9347e97cfa1de0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:10.270482Z",
     "start_time": "2024-10-12T15:32:10.265181Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "47019696734b90b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:10.362043Z",
     "start_time": "2024-10-12T15:32:10.350944Z"
    }
   },
   "cell_type": "code",
   "source": "n_mt_ohlcv.index.dtypes\n",
   "id": "11cbb99e2b29ee8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeframe                 object\n",
       "date         datetime64[ns, UTC]\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f7a72c58508d78d9"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-12T17:21:28.998055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from helper.data_preparation import pattern_timeframe, trigger_timeframe\n",
    "from helper.importer import pt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "def prepare_train_n_test(t_structure_timeframe, mt_ohlcv: pt.DataFrame[MultiTimeframeOHLCV], forecast_horizon: int = 20,\n",
    "                         batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Prepares input and output data for multi-step forecasting.\n",
    "    \n",
    "    Args:\n",
    "        mt_ohlcv: \n",
    "        t_structure_timeframe: \n",
    "        df (pd.DataFrame): DataFrame containing 'high' and 'low' columns.\n",
    "        window_size (int): Number of past time steps to use as input.\n",
    "        forecast_horizon (int): Number of future time steps to predict.\n",
    "        \n",
    "    Returns:\n",
    "        X (np.array): Input features.\n",
    "        y (np.array): Output targets (high and low).\n",
    "    \"\"\"\n",
    "    pattern_tf = pattern_timeframe(t_structure_timeframe)\n",
    "    trigger_tf = trigger_timeframe(t_structure_timeframe)\n",
    "    double_tf = pattern_timeframe(trigger_timeframe(t_structure_timeframe))\n",
    "\n",
    "    structure_df = single_timeframe(mt_ohlcv, t_structure_timeframe)\n",
    "    pattern_df = single_timeframe(mt_ohlcv, pattern_tf)\n",
    "    trigger_df = single_timeframe(mt_ohlcv, trigger_tf)\n",
    "    double_df = single_timeframe(mt_ohlcv, double_tf)\n",
    "\n",
    "    length_of_training = (\n",
    "            model_input_lengths['structure'] * pd.to_timedelta(t_structure_timeframe)\n",
    "            + model_input_lengths['pattern'] * pd.to_timedelta(pattern_tf)\n",
    "            + model_input_lengths['trigger'] * pd.to_timedelta(trigger_tf)\n",
    "            + model_input_lengths['double'] * pd.to_timedelta(double_tf)\n",
    "    )\n",
    "\n",
    "    input_start = mt_ohlcv.index.get_level_values(\n",
    "        level='date').min() + length_of_training * 2  # * 2 for simple safeside.\n",
    "    input_end = mt_ohlcv.index.get_level_values(level='date').max() - forecast_horizon * pd.to_timedelta(\n",
    "        trigger_tf)\n",
    "    duration_seconds = (input_end - input_start) / timedelta(seconds=1)\n",
    "\n",
    "    X, y = {'double': [], 'trigger': [], 'pattern': [], 'structure': [], }, []\n",
    "\n",
    "    for relative_double_end in np.random.randint(0, duration_seconds, size=batch_size):\n",
    "        double_end = input_end - relative_double_end * timedelta(seconds=1)\n",
    "        trigger_end = double_end - model_input_lengths['double'] * pd.to_timedelta(double_tf)\n",
    "        pattern_end = trigger_end - model_input_lengths['trigger'] * pd.to_timedelta(double_tf)\n",
    "        structure_end = pattern_end - model_input_lengths['pattern'] * pd.to_timedelta(double_tf)\n",
    "\n",
    "        double_slice = double_df.loc[pd.IndexSlice[: double_end], :].iloc[-model_input_lengths['double']:]\n",
    "        trigger_slice = trigger_df.loc[pd.IndexSlice[: trigger_end], :].iloc[-model_input_lengths['trigger']:]\n",
    "        pattern_slice = pattern_df.loc[pd.IndexSlice[: pattern_end], :].iloc[-model_input_lengths['pattern']:]\n",
    "        structure_slice = structure_df.loc[pd.IndexSlice[: structure_end], :].iloc[-model_input_lengths['structure']:]\n",
    "\n",
    "        X['double'].append(double_slice)\n",
    "        X['trigger'].append(trigger_slice)\n",
    "        X['pattern'].append(pattern_slice)\n",
    "        X['structure'].append(structure_slice)\n",
    "\n",
    "        future_slice = trigger_df.loc[pd.IndexSlice[pattern_end:], :].iloc[:-model_input_lengths['pattern']]\n",
    "        y.append(future_slice)\n",
    "\n",
    "        min_date = min(double_slice.index.get_level_values(level='date').min(),\n",
    "                       trigger_slice.index.get_level_values(level='date').min(),\n",
    "                       pattern_slice.index.get_level_values(level='date').min(),\n",
    "                       structure_slice.index.get_level_values(level='date').min())\n",
    "        max_date = max(double_slice.index.get_level_values(level='date').max(),\n",
    "                       trigger_slice.index.get_level_values(level='date').max(),\n",
    "                       pattern_slice.index.get_level_values(level='date').max(),\n",
    "                       structure_slice.index.get_level_values(level='date').max())\n",
    "\n",
    "        # Create traces for each slice\n",
    "        double_trace = go.Scatter(x=double_slice.index.get_level_values(level='date'), y=double_slice['value_column'],\n",
    "                                  mode='lines', name='Double')\n",
    "        trigger_trace = go.Scatter(x=trigger_slice.index.get_level_values(level='date'),\n",
    "                                   y=trigger_slice['value_column'], mode='lines', name='Trigger')\n",
    "        pattern_trace = go.Scatter(x=pattern_slice.index.get_level_values(level='date'),\n",
    "                                   y=pattern_slice['value_column'], mode='lines', name='Pattern')\n",
    "        structure_trace = go.Scatter(x=structure_slice.index.get_level_values(level='date'),\n",
    "                                     y=structure_slice['value_column'], mode='lines', name='Structure')\n",
    "\n",
    "        # Create the layout\n",
    "        layout = go.Layout(\n",
    "            title='X Items and Double Slice Over Date Range',\n",
    "            xaxis=dict(title='Date', range=[min_date, max_date]),\n",
    "            yaxis=dict(title='Values'),\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        # Combine all traces into a figure\n",
    "        fig = go.Figure(data=[double_trace, trigger_trace, pattern_trace, structure_trace], layout=layout)\n",
    "\n",
    "        # Show the plot\n",
    "        show_and_save_plot(fig)\n",
    "        nop = 1\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "a = prepare_train_n_test('4h', n_mt_ohlcv, 10)\n",
    "a"
   ],
   "id": "c4fda4bf2a9b30c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "structure_timeframes = {\n",
    "    '1W': {'pattern': '1D', 'trigger': '4h', 'double': '15min'},\n",
    "    '1D': {'pattern': '4h', 'trigger': '1h', 'double': '5min'},\n",
    "    '4h': {'pattern': '1h', 'trigger': '15min', 'double': '1min'}\n",
    "}\n",
    "loop over structure timeframe in ['1D', '4h']:\n",
    "collect information from already prepared function read_ohlcv_features(start, end, timeframe)\n",
    "create required iteration to pass data to train_model\n",
    "choose double_timeframe_end in the range of start and end\n",
    "structure_timeframe_end= structure_timeframe_end = trigger_timeframe_end = double_timeframe_end\n",
    "calculate trigger_timeframe_start according to trigger_timeframe_end and number of bars shall be passed for taining 'trigger_model' \n",
    "calculate pattern_timeframe_start according to pattern_timeframe_end and number of bars shall be passed for taining 'pattern_model' \n",
    "calculate structure_timeframe_start according to structure_timeframe_end and number of bars shall be passed for taining 'structure_model' \n",
    "\n",
    "for:\n",
    "```python\n",
    "def create_cnn_lstm(input_shape, name_prefix):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # CNN Layer with ReLU activation\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same')(input_layer)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same')(conv)\n",
    "    conv = LeakyReLU()(conv)\n",
    "\n",
    "    # Flatten the CNN output\n",
    "    flatten = Flatten()(conv)\n",
    "\n",
    "    # LSTM Layer (LSTM has built-in activations)\n",
    "    lstm = LSTM(64, return_sequences=False)(tf.expand_dims(flatten, axis=1))\n",
    "\n",
    "    # Fully connected layer with ReLU activation\n",
    "    dense = Dense(64)(lstm)\n",
    "    dense = LeakyReLU()(dense)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=dense)\n",
    "\n",
    "def build_model(input_shapes):\n",
    "    structure_model = create_cnn_lstm((128, 5), 'structure_model')\n",
    "    pattern_model = create_cnn_lstm((256, 5), 'pattern_model')\n",
    "    trigger_model = create_cnn_lstm((256, 5), 'trigger_model')\n",
    "    double_model = create_cnn_lstm((256, 5), 'double_model')\n",
    "    \n",
    "    combined_output = Concatenate()(\n",
    "        [structure_model.output, pattern_model.output, trigger_model.output, double_model.output])\n",
    "    \n",
    "    # Add an additional Dense layer with ReLU activation\n",
    "    combined_dense = Dense(128)(combined_output)\n",
    "    combined_dense = LeakyReLU()(combined_dense)\n",
    "    \n",
    "    # Final output layer (for regression tasks, use linear activation; for classification, consider sigmoid/softmax)\n",
    "    final_output = Dense(1, activation='linear')(combined_dense)\n",
    "    \n",
    "    # Define the final model\n",
    "    model = Model(inputs=[structure_model.input, pattern_model.input, trigger_model.input, double_model.input],\n",
    "                  outputs=final_output)\n",
    "    \n",
    "    # Compile the model with mean squared error loss for regression tasks\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "def train_model(structure_data, pattern_data, trigger_data, double_data, target_data, input_shapes, model = None):\n",
    "    '''\n",
    "    Check if the model is already trained or partially trained. If not, build a new model. \n",
    "    Continue training the model and save the trained model to 'cnn_lstm_model.h5' after each session.\n",
    "\n",
    "    Args:\n",
    "        structure_data: Data for the structure timeframe.\n",
    "        pattern_data: Data for the pattern timeframe.\n",
    "        trigger_data: Data for the trigger timeframe.\n",
    "        double_data: Data for the double timeframe.\n",
    "        target_data: The labels or target values for training.\n",
    "        input_shapes: A dictionary containing the input shapes for structure, pattern, trigger, and double timeframe data.\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    '''\n",
    "    # Check if the model already exists, load if it does\n",
    "    model_path = 'cnn_lstm_model.h5'\n",
    "    \n",
    "    if model is None:\n",
    "        if os.path.exists(model_path):\n",
    "            print(\"Loading existing model from disk...\")\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print(\"Building new model...\")\n",
    "            model = build_model(input_shapes)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([structure_data, pattern_data, trigger_data, double_data],\n",
    "                        target_data,\n",
    "                        epochs=10,\n",
    "                        batch_size=32)\n",
    "    print(history)\n",
    "    # Save the model after each training session to avoid losing progress\n",
    "    model.save(model_path)\n",
    "    print(\"Model saved to disk.\")\n",
    "    \n",
    "    return model\n",
    "```"
   ],
   "id": "8976c18cf541c107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "22c671f910aa6071"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "structure_timeframes = {\n",
    "    '1W': {'pattern': '1D', 'trigger': '4h', 'double': '15min'},\n",
    "    '1D': {'pattern': '4h', 'trigger': '1h', 'double': '5min'},\n",
    "    '4h': {'pattern': '1h', 'trigger': '15min', 'double': '1min'}\n",
    "}\n",
    "\n",
    "\n",
    "def "
   ],
   "id": "96dace33290dc1cf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
